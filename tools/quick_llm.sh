#!/bin/bash
# Quick LLM Companion Launcher

clear
echo "ğŸ¤– QUICK LLM COMPANION"
echo "====================="
echo ""
echo "Choose an option:"
echo "1. ğŸŒ Open Web Interface (http://localhost:3000)"
echo "2. ğŸ’¬ Start Terminal Chat"
echo "3. ğŸ”„ Start Floating Button"
echo "4. ğŸ“Š Check Status"
echo "5. âŒ Exit"
echo ""
read -p "Enter choice (1-5): " choice

case $choice in
    1)
        echo "ğŸŒ Opening web interface..."
        xdg-open http://localhost:3000 2>/dev/null || firefox http://localhost:3000 2>/dev/null || google-chrome http://localhost:3000 2>/dev/null
        ;;
    2)
        echo "ğŸ’¬ Starting terminal chat..."
        ai-chat
        ;;
    3)
        echo "ï¿½ï¿½ Starting floating button..."
        python3 simple_floating_button.py &
        echo "âœ… Floating button started - look for it on your screen!"
        ;;
    4)
        echo "ğŸ“Š Checking LLM status..."
        echo ""
        echo "Ollama: $(systemctl is-active ollama 2>/dev/null || echo 'inactive')"
        echo "WebUI: $(docker ps --format 'table {{.Status}}' | grep webui || echo 'Not running')"
        echo ""
        echo "Available models:"
        ollama list 2>/dev/null || echo "No models found"
        echo ""
        read -p "Press Enter to continue..."
        ;;
    5)
        echo "ğŸ‘‹ Goodbye!"
        exit 0
        ;;
    *)
        echo "âŒ Invalid choice"
        ;;
esac
