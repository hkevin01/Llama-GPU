{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e624d3e",
   "metadata": {},
   "source": [
    "# Opik Client Libraries and REST API Integration Demo\n",
    "\n",
    "This notebook demonstrates how to use Opik's suite of client libraries and REST API for seamless integration into your ML workflows. Opik provides SDKs for Python, TypeScript, and Ruby (via OpenTelemetry) for comprehensive experiment tracking and monitoring.\n",
    "\n",
    "## Overview\n",
    "- Install and configure Opik client\n",
    "- Basic API operations and CRUD functionality  \n",
    "- Python SDK usage examples\n",
    "- Logging and tracking operations\n",
    "- Error handling and best practices\n",
    "\n",
    "For detailed API and SDK references, see the [Opik Client Reference Documentation](https://docs.opik.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d255d83",
   "metadata": {},
   "source": [
    "## 1. Install and Import Opik\n",
    "\n",
    "First, we'll install the Opik package and import the necessary modules for working with the Opik client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c108f29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Opik package\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "try:\n",
    "    import opik\n",
    "    print(\"✓ Opik already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing Opik...\")\n",
    "    install_package(\"opik\")\n",
    "    import opik\n",
    "    print(\"✓ Opik installed successfully\")\n",
    "\n",
    "# Import necessary modules\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"Opik version: {opik.__version__}\")\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f8a2ef",
   "metadata": {},
   "source": [
    "## 2. Initialize Opik Client\n",
    "\n",
    "Set up and configure the Opik client with proper authentication and server connection settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9117a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Opik client\n",
    "from opik import Opik\n",
    "from opik.config import OpikConfig\n",
    "\n",
    "# Configuration options\n",
    "config = {\n",
    "    \"api_key\": os.getenv(\"OPIK_API_KEY\", \"your-api-key-here\"),\n",
    "    \"workspace\": os.getenv(\"OPIK_WORKSPACE\", \"default\"),\n",
    "    \"url\": os.getenv(\"OPIK_URL\", \"https://www.comet.com/opik/api\"),\n",
    "}\n",
    "\n",
    "print(\"Initializing Opik client...\")\n",
    "print(f\"Workspace: {config['workspace']}\")\n",
    "print(f\"URL: {config['url']}\")\n",
    "\n",
    "try:\n",
    "    # Initialize Opik client\n",
    "    client = Opik(\n",
    "        api_key=config[\"api_key\"],\n",
    "        workspace=config[\"workspace\"],\n",
    "        url=config[\"url\"]\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Opik client initialized successfully\")\n",
    "    \n",
    "    # Test connection\n",
    "    try:\n",
    "        # This would test the connection (adjust based on actual Opik API)\n",
    "        info = client.get_workspace_info()\n",
    "        print(f\"✓ Connection successful - Workspace: {info.get('name', 'N/A')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Connection test failed (this may be expected in demo): {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to initialize Opik client: {e}\")\n",
    "    print(\"Note: Make sure you have valid credentials set in environment variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b850a0f",
   "metadata": {},
   "source": [
    "## 3. Basic API Operations\n",
    "\n",
    "Demonstrate basic CRUD operations using the Opik REST API, including creating, reading, updating, and deleting resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8e64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic API Operations using Opik REST API\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# API endpoints (adjust based on actual Opik API structure)\n",
    "base_url = config[\"url\"]\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {config['api_key']}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def make_api_request(method, endpoint, data=None):\n",
    "    \"\"\"Make a request to the Opik API\"\"\"\n",
    "    url = f\"{base_url.rstrip('/')}/{endpoint.lstrip('/')}\"\n",
    "    \n",
    "    try:\n",
    "        if method.upper() == \"GET\":\n",
    "            response = requests.get(url, headers=headers)\n",
    "        elif method.upper() == \"POST\":\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "        elif method.upper() == \"PUT\":\n",
    "            response = requests.put(url, headers=headers, json=data)\n",
    "        elif method.upper() == \"DELETE\":\n",
    "            response = requests.delete(url, headers=headers)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported HTTP method: {method}\")\n",
    "            \n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example: Create a new experiment (adjust endpoint based on actual API)\n",
    "print(\"=== CREATE Operation ===\")\n",
    "experiment_data = {\n",
    "    \"name\": \"llama-gpu-experiment\",\n",
    "    \"description\": \"Test experiment for LLaMA GPU optimization\",\n",
    "    \"tags\": [\"llama\", \"gpu\", \"optimization\"],\n",
    "    \"created_at\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "create_response = make_api_request(\"POST\", \"/experiments\", experiment_data)\n",
    "if create_response:\n",
    "    print(f\"Create response status: {create_response.status_code}\")\n",
    "    if create_response.status_code < 400:\n",
    "        print(\"✓ Experiment created successfully\")\n",
    "        created_experiment = create_response.json()\n",
    "        experiment_id = created_experiment.get(\"id\", \"demo-id\")\n",
    "    else:\n",
    "        print(f\"✗ Create failed: {create_response.text}\")\n",
    "        experiment_id = \"demo-id\"  # Fallback for demo\n",
    "else:\n",
    "    print(\"⚠ Create request failed (network/auth issue)\")\n",
    "    experiment_id = \"demo-id\"\n",
    "\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "\n",
    "# Example: Read (GET) operation\n",
    "print(\"\\n=== READ Operation ===\")\n",
    "get_response = make_api_request(\"GET\", f\"/experiments/{experiment_id}\")\n",
    "if get_response and get_response.status_code < 400:\n",
    "    print(\"✓ Experiment retrieved successfully\")\n",
    "    experiment = get_response.json()\n",
    "    print(f\"Name: {experiment.get('name', 'N/A')}\")\n",
    "    print(f\"Description: {experiment.get('description', 'N/A')}\")\n",
    "else:\n",
    "    print(\"⚠ Read operation demo (actual API may differ)\")\n",
    "\n",
    "# Example: Update (PUT) operation\n",
    "print(\"\\n=== UPDATE Operation ===\")\n",
    "update_data = {\n",
    "    \"description\": \"Updated: LLaMA GPU optimization with monitoring\",\n",
    "    \"tags\": [\"llama\", \"gpu\", \"optimization\", \"monitoring\"]\n",
    "}\n",
    "\n",
    "update_response = make_api_request(\"PUT\", f\"/experiments/{experiment_id}\", update_data)\n",
    "if update_response and update_response.status_code < 400:\n",
    "    print(\"✓ Experiment updated successfully\")\n",
    "else:\n",
    "    print(\"⚠ Update operation demo (actual API may differ)\")\n",
    "\n",
    "# Example: Delete operation (commented out to preserve demo data)\n",
    "print(\"\\n=== DELETE Operation ===\")\n",
    "print(\"⚠ Delete operation available but not executed in demo\")\n",
    "# delete_response = make_api_request(\"DELETE\", f\"/experiments/{experiment_id}\")\n",
    "# if delete_response and delete_response.status_code < 400:\n",
    "#     print(\"✓ Experiment deleted successfully\")\n",
    "\n",
    "print(\"\\n✓ Basic CRUD operations demonstrated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9872618f",
   "metadata": {},
   "source": [
    "## 4. Python SDK Usage Examples\n",
    "\n",
    "Show practical examples of using the Opik Python SDK for common tasks like data logging, experiment tracking, and metric collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5bf81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python SDK Usage Examples\n",
    "from opik import track\n",
    "from opik.decorators import op\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Example 1: Basic tracking decorator\n",
    "@track(project_name=\"llama-gpu-optimization\")\n",
    "def train_model(model_name, epochs, learning_rate):\n",
    "    \"\"\"Simulate model training with tracking\"\"\"\n",
    "    print(f\"Training {model_name} for {epochs} epochs...\")\n",
    "    \n",
    "    # Simulate training process\n",
    "    for epoch in range(epochs):\n",
    "        # Simulate training metrics\n",
    "        loss = random.uniform(0.5, 2.0) * (0.9 ** epoch)  # Decreasing loss\n",
    "        accuracy = min(0.95, 0.6 + (epoch * 0.05))  # Increasing accuracy\n",
    "        \n",
    "        # Log metrics for this epoch\n",
    "        if hasattr(track.current_span(), 'log_metric'):\n",
    "            track.current_span().log_metric(\"loss\", loss, step=epoch)\n",
    "            track.current_span().log_metric(\"accuracy\", accuracy, step=epoch)\n",
    "            track.current_span().log_metric(\"learning_rate\", learning_rate, step=epoch)\n",
    "        \n",
    "        time.sleep(0.1)  # Simulate training time\n",
    "    \n",
    "    # Final results\n",
    "    final_metrics = {\n",
    "        \"final_loss\": loss,\n",
    "        \"final_accuracy\": accuracy,\n",
    "        \"total_epochs\": epochs,\n",
    "        \"model_name\": model_name\n",
    "    }\n",
    "    \n",
    "    print(f\"Training complete! Final accuracy: {accuracy:.3f}\")\n",
    "    return final_metrics\n",
    "\n",
    "# Example 2: Manual span creation\n",
    "def evaluate_model(model_name, test_size):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    with track.start_span(\n",
    "        name=\"model_evaluation\",\n",
    "        input={\"model_name\": model_name, \"test_size\": test_size}\n",
    "    ) as span:\n",
    "        \n",
    "        # Simulate evaluation\n",
    "        print(f\"Evaluating {model_name} on {test_size} samples...\")\n",
    "        \n",
    "        # Simulate evaluation metrics\n",
    "        precision = random.uniform(0.8, 0.95)\n",
    "        recall = random.uniform(0.75, 0.92)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        evaluation_results = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"test_size\": test_size\n",
    "        }\n",
    "        \n",
    "        # Log evaluation results\n",
    "        span.set_output(evaluation_results)\n",
    "        span.log_metric(\"precision\", precision)\n",
    "        span.log_metric(\"recall\", recall)\n",
    "        span.log_metric(\"f1_score\", f1_score)\n",
    "        \n",
    "        print(f\"Evaluation complete! F1-score: {f1_score:.3f}\")\n",
    "        return evaluation_results\n",
    "\n",
    "# Example 3: GPU optimization tracking\n",
    "@op(name=\"gpu_optimization\")\n",
    "def optimize_gpu_settings(gpu_memory_gb, batch_size, precision_mode):\n",
    "    \"\"\"Track GPU optimization experiments\"\"\"\n",
    "    \n",
    "    optimization_config = {\n",
    "        \"gpu_memory_gb\": gpu_memory_gb,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"precision_mode\": precision_mode\n",
    "    }\n",
    "    \n",
    "    print(f\"Optimizing GPU settings: {optimization_config}\")\n",
    "    \n",
    "    # Simulate optimization process\n",
    "    throughput = random.uniform(50, 200)  # tokens/second\n",
    "    memory_usage = random.uniform(0.7, 0.95) * gpu_memory_gb\n",
    "    power_consumption = random.uniform(150, 300)  # watts\n",
    "    \n",
    "    optimization_results = {\n",
    "        \"throughput_tokens_per_sec\": throughput,\n",
    "        \"memory_usage_gb\": memory_usage,\n",
    "        \"power_consumption_watts\": power_consumption,\n",
    "        \"efficiency_score\": throughput / power_consumption\n",
    "    }\n",
    "    \n",
    "    print(f\"Optimization results: {optimization_results}\")\n",
    "    return optimization_results\n",
    "\n",
    "# Run examples\n",
    "print(\"=== Running Python SDK Examples ===\")\n",
    "\n",
    "# Example 1: Train a model\n",
    "print(\"\\n1. Training model with tracking...\")\n",
    "training_results = train_model(\n",
    "    model_name=\"llama-7b-gpu\",\n",
    "    epochs=5,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Example 2: Evaluate model\n",
    "print(\"\\n2. Evaluating model...\")\n",
    "evaluation_results = evaluate_model(\n",
    "    model_name=\"llama-7b-gpu\",\n",
    "    test_size=1000\n",
    ")\n",
    "\n",
    "# Example 3: GPU optimization\n",
    "print(\"\\n3. GPU optimization...\")\n",
    "gpu_results = optimize_gpu_settings(\n",
    "    gpu_memory_gb=24,\n",
    "    batch_size=32,\n",
    "    precision_mode=\"fp16\"\n",
    ")\n",
    "\n",
    "print(\"\\n✓ All SDK examples completed successfully\")\n",
    "print(\"Check your Opik dashboard for tracked experiments and metrics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edffe61b",
   "metadata": {},
   "source": [
    "## 5. Logging and Tracking Operations\n",
    "\n",
    "Implement logging functionality to track experiments, metrics, and other relevant data using Opik's tracking capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Logging and Tracking Operations\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import base64\n",
    "\n",
    "class LlamaGPUTracker:\n",
    "    \"\"\"Enhanced tracking class for LLaMA GPU experiments\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name=\"llama-gpu-experiments\"):\n",
    "        self.project_name = project_name\n",
    "        self.current_experiment = None\n",
    "        \n",
    "    def start_experiment(self, experiment_name, config=None):\n",
    "        \"\"\"Start a new experiment with configuration\"\"\"\n",
    "        self.current_experiment = {\n",
    "            \"name\": experiment_name,\n",
    "            \"config\": config or {},\n",
    "            \"metrics\": {},\n",
    "            \"artifacts\": {},\n",
    "            \"start_time\": datetime.now()\n",
    "        }\n",
    "        \n",
    "        print(f\"Started experiment: {experiment_name}\")\n",
    "        if config:\n",
    "            print(f\"Configuration: {json.dumps(config, indent=2)}\")\n",
    "    \n",
    "    def log_metrics(self, metrics_dict, step=None):\n",
    "        \"\"\"Log multiple metrics at once\"\"\"\n",
    "        if not self.current_experiment:\n",
    "            print(\"Warning: No active experiment\")\n",
    "            return\n",
    "            \n",
    "        for metric_name, value in metrics_dict.items():\n",
    "            if metric_name not in self.current_experiment[\"metrics\"]:\n",
    "                self.current_experiment[\"metrics\"][metric_name] = []\n",
    "            \n",
    "            self.current_experiment[\"metrics\"][metric_name].append({\n",
    "                \"value\": value,\n",
    "                \"step\": step,\n",
    "                \"timestamp\": datetime.now()\n",
    "            })\n",
    "    \n",
    "    def log_gpu_metrics(self, gpu_stats):\n",
    "        \"\"\"Log GPU-specific metrics\"\"\"\n",
    "        gpu_metrics = {\n",
    "            \"gpu_memory_used\": gpu_stats.get(\"memory_used\", 0),\n",
    "            \"gpu_memory_total\": gpu_stats.get(\"memory_total\", 0),\n",
    "            \"gpu_utilization\": gpu_stats.get(\"utilization\", 0),\n",
    "            \"gpu_temperature\": gpu_stats.get(\"temperature\", 0),\n",
    "            \"power_draw\": gpu_stats.get(\"power_draw\", 0)\n",
    "        }\n",
    "        \n",
    "        self.log_metrics(gpu_metrics)\n",
    "        print(f\"Logged GPU metrics: {gpu_metrics}\")\n",
    "    \n",
    "    def log_model_metrics(self, model_stats):\n",
    "        \"\"\"Log model-specific metrics\"\"\"\n",
    "        model_metrics = {\n",
    "            \"tokens_per_second\": model_stats.get(\"throughput\", 0),\n",
    "            \"perplexity\": model_stats.get(\"perplexity\", 0),\n",
    "            \"bleu_score\": model_stats.get(\"bleu_score\", 0),\n",
    "            \"latency_ms\": model_stats.get(\"latency_ms\", 0)\n",
    "        }\n",
    "        \n",
    "        self.log_metrics(model_metrics)\n",
    "        print(f\"Logged model metrics: {model_metrics}\")\n",
    "    \n",
    "    def log_artifact(self, name, content, artifact_type=\"text\"):\n",
    "        \"\"\"Log artifacts like plots, configs, or text files\"\"\"\n",
    "        if not self.current_experiment:\n",
    "            print(\"Warning: No active experiment\")\n",
    "            return\n",
    "            \n",
    "        self.current_experiment[\"artifacts\"][name] = {\n",
    "            \"content\": content,\n",
    "            \"type\": artifact_type,\n",
    "            \"timestamp\": datetime.now()\n",
    "        }\n",
    "        \n",
    "        print(f\"Logged artifact: {name} ({artifact_type})\")\n",
    "    \n",
    "    def create_performance_plot(self):\n",
    "        \"\"\"Create and log a performance visualization\"\"\"\n",
    "        if not self.current_experiment or \"tokens_per_second\" not in self.current_experiment[\"metrics\"]:\n",
    "            print(\"No performance data to plot\")\n",
    "            return\n",
    "            \n",
    "        # Extract throughput data\n",
    "        throughput_data = self.current_experiment[\"metrics\"][\"tokens_per_second\"]\n",
    "        values = [entry[\"value\"] for entry in throughput_data]\n",
    "        steps = list(range(len(values)))\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(steps, values, 'b-', linewidth=2, label='Tokens/Second')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Throughput (tokens/sec)')\n",
    "        plt.title('Model Performance Over Time')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Save plot to buffer\n",
    "        buffer = io.BytesIO()\n",
    "        plt.savefig(buffer, format='png', dpi=150, bbox_inches='tight')\n",
    "        buffer.seek(0)\n",
    "        plot_data = base64.b64encode(buffer.getvalue()).decode()\n",
    "        plt.close()\n",
    "        \n",
    "        # Log as artifact\n",
    "        self.log_artifact(\"performance_plot\", plot_data, \"image\")\n",
    "        print(\"Created and logged performance plot\")\n",
    "    \n",
    "    def end_experiment(self):\n",
    "        \"\"\"End the current experiment and summarize results\"\"\"\n",
    "        if not self.current_experiment:\n",
    "            print(\"No active experiment to end\")\n",
    "            return\n",
    "            \n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - self.current_experiment[\"start_time\"]\n",
    "        \n",
    "        print(f\"\\n=== Experiment Summary: {self.current_experiment['name']} ===\")\n",
    "        print(f\"Duration: {duration}\")\n",
    "        print(f\"Metrics logged: {len(self.current_experiment['metrics'])}\")\n",
    "        print(f\"Artifacts created: {len(self.current_experiment['artifacts'])}\")\n",
    "        \n",
    "        # Log final summary metrics\n",
    "        for metric_name, entries in self.current_experiment[\"metrics\"].items():\n",
    "            if entries:\n",
    "                latest_value = entries[-1][\"value\"]\n",
    "                print(f\"Final {metric_name}: {latest_value}\")\n",
    "        \n",
    "        self.current_experiment = None\n",
    "        print(\"Experiment ended successfully\")\n",
    "\n",
    "# Demo: Comprehensive experiment tracking\n",
    "print(\"=== Comprehensive Experiment Tracking Demo ===\")\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = LlamaGPUTracker(\"llama-gpu-optimization\")\n",
    "\n",
    "# Start experiment\n",
    "tracker.start_experiment(\n",
    "    \"llama-7b-inference-optimization\",\n",
    "    config={\n",
    "        \"model_size\": \"7B\",\n",
    "        \"precision\": \"fp16\",\n",
    "        \"batch_size\": 32,\n",
    "        \"max_length\": 2048,\n",
    "        \"gpu_model\": \"RTX 4090\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Simulate training/inference loop with comprehensive logging\n",
    "print(\"\\nSimulating inference optimization...\")\n",
    "for step in range(10):\n",
    "    # Simulate GPU metrics\n",
    "    gpu_stats = {\n",
    "        \"memory_used\": random.uniform(18, 22),  # GB\n",
    "        \"memory_total\": 24,\n",
    "        \"utilization\": random.uniform(85, 98),  # %\n",
    "        \"temperature\": random.uniform(65, 82),  # °C\n",
    "        \"power_draw\": random.uniform(280, 350)  # watts\n",
    "    }\n",
    "    \n",
    "    # Simulate model performance metrics\n",
    "    model_stats = {\n",
    "        \"throughput\": random.uniform(50, 120),  # tokens/sec\n",
    "        \"perplexity\": random.uniform(3.2, 4.8),\n",
    "        \"bleu_score\": random.uniform(0.25, 0.45),\n",
    "        \"latency_ms\": random.uniform(15, 35)\n",
    "    }\n",
    "    \n",
    "    # Log all metrics\n",
    "    tracker.log_gpu_metrics(gpu_stats)\n",
    "    tracker.log_model_metrics(model_stats)\n",
    "    \n",
    "    # Log additional custom metrics\n",
    "    custom_metrics = {\n",
    "        \"step\": step,\n",
    "        \"efficiency_score\": model_stats[\"throughput\"] / gpu_stats[\"power_draw\"],\n",
    "        \"memory_efficiency\": model_stats[\"throughput\"] / gpu_stats[\"memory_used\"]\n",
    "    }\n",
    "    tracker.log_metrics(custom_metrics, step=step)\n",
    "    \n",
    "    time.sleep(0.2)  # Simulate processing time\n",
    "\n",
    "# Create performance visualization\n",
    "tracker.create_performance_plot()\n",
    "\n",
    "# Log configuration file as artifact\n",
    "config_content = json.dumps({\n",
    "    \"model_config\": {\n",
    "        \"architecture\": \"llama\",\n",
    "        \"size\": \"7B\",\n",
    "        \"precision\": \"fp16\"\n",
    "    },\n",
    "    \"optimization_settings\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"max_length\": 2048,\n",
    "        \"attention_optimization\": True\n",
    "    }\n",
    "}, indent=2)\n",
    "\n",
    "tracker.log_artifact(\"model_config.json\", config_content, \"config\")\n",
    "\n",
    "# End experiment\n",
    "tracker.end_experiment()\n",
    "\n",
    "print(\"\\n✓ Comprehensive logging and tracking demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb813d96",
   "metadata": {},
   "source": [
    "## 6. Error Handling and Best Practices\n",
    "\n",
    "Demonstrate proper error handling techniques and best practices when working with the Opik client libraries and API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aab27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Handling and Best Practices\n",
    "import functools\n",
    "import traceback\n",
    "from typing import Optional, Dict, Any\n",
    "import time\n",
    "\n",
    "class OpikErrorHandler:\n",
    "    \"\"\"Centralized error handling for Opik operations\"\"\"\n",
    "    \n",
    "    def __init__(self, max_retries=3, backoff_factor=1.5):\n",
    "        self.max_retries = max_retries\n",
    "        self.backoff_factor = backoff_factor\n",
    "        self.error_log = []\n",
    "    \n",
    "    def log_error(self, error, context=None):\n",
    "        \"\"\"Log error with context information\"\"\"\n",
    "        error_entry = {\n",
    "            \"error\": str(error),\n",
    "            \"type\": type(error).__name__,\n",
    "            \"context\": context or {},\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"traceback\": traceback.format_exc()\n",
    "        }\n",
    "        self.error_log.append(error_entry)\n",
    "        logger.error(f\"Opik operation failed: {error}\")\n",
    "    \n",
    "    def retry_with_backoff(self, func):\n",
    "        \"\"\"Decorator for retrying operations with exponential backoff\"\"\"\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            last_exception = None\n",
    "            \n",
    "            for attempt in range(self.max_retries):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    last_exception = e\n",
    "                    wait_time = self.backoff_factor ** attempt\n",
    "                    \n",
    "                    self.log_error(e, {\n",
    "                        \"function\": func.__name__,\n",
    "                        \"attempt\": attempt + 1,\n",
    "                        \"max_retries\": self.max_retries\n",
    "                    })\n",
    "                    \n",
    "                    if attempt < self.max_retries - 1:\n",
    "                        print(f\"Attempt {attempt + 1} failed, retrying in {wait_time:.1f}s...\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(f\"All {self.max_retries} attempts failed\")\n",
    "            \n",
    "            raise last_exception\n",
    "        return wrapper\n",
    "\n",
    "# Initialize error handler\n",
    "error_handler = OpikErrorHandler()\n",
    "\n",
    "@error_handler.retry_with_backoff\n",
    "def robust_api_call(endpoint, data=None, method=\"GET\"):\n",
    "    \"\"\"Example of a robust API call with error handling\"\"\"\n",
    "    \n",
    "    # Validate inputs\n",
    "    if not endpoint:\n",
    "        raise ValueError(\"Endpoint cannot be empty\")\n",
    "    \n",
    "    if method not in [\"GET\", \"POST\", \"PUT\", \"DELETE\"]:\n",
    "        raise ValueError(f\"Unsupported HTTP method: {method}\")\n",
    "    \n",
    "    # Simulate API call that might fail\n",
    "    if random.random() < 0.3:  # 30% chance of failure for demo\n",
    "        raise requests.exceptions.ConnectionError(\"Simulated network error\")\n",
    "    \n",
    "    if random.random() < 0.2:  # 20% chance of auth error for demo\n",
    "        raise requests.exceptions.HTTPError(\"401 Unauthorized\")\n",
    "    \n",
    "    # Simulate successful response\n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"endpoint\": endpoint,\n",
    "        \"method\": method,\n",
    "        \"data\": data\n",
    "    }\n",
    "\n",
    "@error_handler.retry_with_backoff\n",
    "def robust_metric_logging(metric_name, value, experiment_id=None):\n",
    "    \"\"\"Robust metric logging with validation and error handling\"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    if not metric_name or not isinstance(metric_name, str):\n",
    "        raise ValueError(\"Metric name must be a non-empty string\")\n",
    "    \n",
    "    if not isinstance(value, (int, float)) or np.isnan(value) or np.isinf(value):\n",
    "        raise ValueError(f\"Invalid metric value: {value}\")\n",
    "    \n",
    "    # Simulate logging operation\n",
    "    if random.random() < 0.15:  # 15% chance of failure\n",
    "        raise Exception(\"Failed to log metric to Opik server\")\n",
    "    \n",
    "    print(f\"✓ Logged metric: {metric_name} = {value}\")\n",
    "    return True\n",
    "\n",
    "def safe_experiment_context(experiment_name, config=None):\n",
    "    \"\"\"Context manager for safe experiment execution\"\"\"\n",
    "    experiment_started = False\n",
    "    \n",
    "    try:\n",
    "        # Start experiment\n",
    "        print(f\"Starting experiment: {experiment_name}\")\n",
    "        experiment_started = True\n",
    "        \n",
    "        # Yield control to user code\n",
    "        yield {\n",
    "            \"experiment_name\": experiment_name,\n",
    "            \"config\": config or {},\n",
    "            \"start_time\": datetime.now()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_handler.log_error(e, {\n",
    "            \"experiment_name\": experiment_name,\n",
    "            \"config\": config\n",
    "        })\n",
    "        print(f\"✗ Experiment failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        if experiment_started:\n",
    "            print(f\"Cleaning up experiment: {experiment_name}\")\n",
    "            # Perform cleanup operations here\n",
    "\n",
    "def validate_opik_config(config):\n",
    "    \"\"\"Validate Opik configuration\"\"\"\n",
    "    required_fields = [\"api_key\", \"workspace\"]\n",
    "    errors = []\n",
    "    \n",
    "    for field in required_fields:\n",
    "        if not config.get(field):\n",
    "            errors.append(f\"Missing required field: {field}\")\n",
    "    \n",
    "    if config.get(\"api_key\") == \"your-api-key-here\":\n",
    "        errors.append(\"Please set a valid API key\")\n",
    "    \n",
    "    if errors:\n",
    "        raise ValueError(f\"Configuration errors: {', '.join(errors)}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Best Practices Examples\n",
    "print(\"=== Error Handling and Best Practices Demo ===\")\n",
    "\n",
    "# 1. Configuration validation\n",
    "print(\"\\n1. Configuration Validation:\")\n",
    "try:\n",
    "    test_config = {\n",
    "        \"api_key\": \"your-api-key-here\",  # Invalid placeholder\n",
    "        \"workspace\": \"test-workspace\"\n",
    "    }\n",
    "    validate_opik_config(test_config)\n",
    "except ValueError as e:\n",
    "    print(f\"✓ Configuration validation caught error: {e}\")\n",
    "\n",
    "# Valid configuration\n",
    "valid_config = {\n",
    "    \"api_key\": \"valid-test-key-123\",\n",
    "    \"workspace\": \"test-workspace\",\n",
    "    \"url\": \"https://api.opik.io\"\n",
    "}\n",
    "validate_opik_config(valid_config)\n",
    "print(\"✓ Configuration validation passed\")\n",
    "\n",
    "# 2. Robust API calls with retry\n",
    "print(\"\\n2. Robust API Calls with Retry:\")\n",
    "try:\n",
    "    result = robust_api_call(\"/experiments\", {\"name\": \"test\"}, \"POST\")\n",
    "    print(f\"✓ API call succeeded: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ API call failed after all retries: {e}\")\n",
    "\n",
    "# 3. Safe metric logging\n",
    "print(\"\\n3. Safe Metric Logging:\")\n",
    "test_metrics = [\n",
    "    (\"valid_metric\", 0.85),\n",
    "    (\"invalid_metric\", float('nan')),  # Should fail validation\n",
    "    (\"another_valid\", 42.0)\n",
    "]\n",
    "\n",
    "for metric_name, value in test_metrics:\n",
    "    try:\n",
    "        robust_metric_logging(metric_name, value)\n",
    "    except ValueError as e:\n",
    "        print(f\"✓ Validation caught invalid metric: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Unexpected error: {e}\")\n",
    "\n",
    "# 4. Safe experiment context\n",
    "print(\"\\n4. Safe Experiment Context:\")\n",
    "try:\n",
    "    with safe_experiment_context(\"error-handling-demo\") as experiment:\n",
    "        print(f\"Running experiment: {experiment['experiment_name']}\")\n",
    "        \n",
    "        # Simulate some operations\n",
    "        for i in range(3):\n",
    "            robust_metric_logging(f\"step_{i}\", random.uniform(0, 1))\n",
    "        \n",
    "        # Simulate an error (uncomment to test error handling)\n",
    "        # raise RuntimeError(\"Simulated experiment error\")\n",
    "        \n",
    "        print(\"✓ Experiment completed successfully\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Experiment handled error gracefully: {e}\")\n",
    "\n",
    "# 5. Error summary and recommendations\n",
    "print(\"\\n=== Error Summary and Recommendations ===\")\n",
    "print(f\"Total errors logged: {len(error_handler.error_log)}\")\n",
    "\n",
    "if error_handler.error_log:\n",
    "    print(\"\\nError types encountered:\")\n",
    "    error_types = {}\n",
    "    for error in error_handler.error_log:\n",
    "        error_type = error[\"type\"]\n",
    "        error_types[error_type] = error_types.get(error_type, 0) + 1\n",
    "    \n",
    "    for error_type, count in error_types.items():\n",
    "        print(f\"  {error_type}: {count} occurrences\")\n",
    "\n",
    "print(\"\\n=== Best Practices Summary ===\")\n",
    "print(\"✓ Always validate configuration before initializing client\")\n",
    "print(\"✓ Implement retry logic with exponential backoff\")\n",
    "print(\"✓ Validate metric values before logging\")\n",
    "print(\"✓ Use context managers for experiment lifecycle\")\n",
    "print(\"✓ Log errors with context for debugging\")\n",
    "print(\"✓ Implement graceful degradation for non-critical operations\")\n",
    "print(\"✓ Monitor API rate limits and implement backoff\")\n",
    "print(\"✓ Use environment variables for sensitive configuration\")\n",
    "\n",
    "print(\"\\n✓ Error handling and best practices demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149f2e35",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "This notebook demonstrated comprehensive integration with Opik's client libraries and REST API for seamless ML workflow integration. \n",
    "\n",
    "### Key Features Covered:\n",
    "- ✅ Package installation and client initialization\n",
    "- ✅ Basic CRUD operations via REST API\n",
    "- ✅ Python SDK decorators and tracking\n",
    "- ✅ Advanced logging and metrics collection\n",
    "- ✅ Error handling and best practices\n",
    "- ✅ GPU optimization tracking\n",
    "- ✅ Experiment lifecycle management\n",
    "\n",
    "### Next Steps:\n",
    "1. **Set up your Opik account** and get real API credentials\n",
    "2. **Configure environment variables** for secure credential management\n",
    "3. **Integrate with your LLaMA GPU project** using the patterns shown\n",
    "4. **Explore TypeScript/Ruby SDKs** for multi-language support\n",
    "5. **Review the [Opik Documentation](https://docs.opik.io/)** for advanced features\n",
    "\n",
    "### Integration with LLaMA GPU Project:\n",
    "- Use tracking decorators in your training/inference code\n",
    "- Log GPU metrics during model optimization\n",
    "- Track experiment configurations and results\n",
    "- Monitor model performance across different hardware setups\n",
    "- Create visualizations for performance analysis\n",
    "\n",
    "For production use, remember to:\n",
    "- Set proper API keys and workspace configurations\n",
    "- Implement proper error handling and retry logic\n",
    "- Monitor API usage and rate limits\n",
    "- Use environment variables for configuration\n",
    "- Set up proper logging and monitoring"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
