{
  "name": "llama-gpu",
  "version": "1.0.0",
  "description": "Modular, GPU-accelerated LLaMA project with cloud deployment, dashboard, plugins, and benchmarking.",
  "main": "src/backend/main.py",
  "scripts": {
    "test": "pytest tests/",
    "lint": "flake8 src/ tests/",
    "start": "python3 src/dashboard.py"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/hkevin01/Llama-GPU.git"
  },
  "author": "hkevin01",
  "license": "MIT",
  "dependencies": {
    "flask": "^2.0.0",
    "torch": "^2.0.0",
    "onnx": "^1.15.0",
    "tensorflow": "^2.15.0"
  },
  "devDependencies": {
    "pytest": "^7.0.0",
    "flake8": "^6.0.0"
  }
}
